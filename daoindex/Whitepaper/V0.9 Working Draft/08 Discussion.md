---
date created: 2023-10-26 17:46
---
# Notes
## Assumptions and Limitations

### Notes
3. DAO definition (or DAO conceptualization)
	1. Is there a need for a definition (or can we simply focus on practices (or can we define daos based on their practices rather than imaginary or conceptualizations)?
	   1. [The Open Source Definition](https://opensource.org/osd/)
	   2. [Digital Public Goods Standard](https://digitalpublicgoods.net/standard/)
	   3. <https://talentdao.substack.com/p/nodw-16-developing-the-dao-health>
	   4. [Location-based Platform Work Principles](https://fair.work/en/fw/principles/fairwork-principles-location-based-work/)
	   5. <https://web.archive.org/web/20201127013501/https://www.ica.coop/en/cooperatives/cooperative-identity>
	   6. [Data Cooperatives for Pandemic Times - Public Seminar](https://publicseminar.org/essays/data-cooperatives-for-pandemic-times/)
		  1. “To avoid mission creep, data cooperatives need to adhere to the globally shared definition of what makes an organization a cooperative:”
	2. Definition should be broad enough to allow others to more narrowly define DAOs for their local contexts?
1. Outside-in view of assessment
	1. Reliance on publicly available information
2. Approach may be inadequate
3. Principles chosen may not be representative of ideals for how a DAO should be operated and governed
4. absence of standardized documentation (like SEC's K-10)
5. absence of measurements of crucial metrics (at least publicly)
6. small dataset
7. lack of systematic research approach to developing principles and questionnaire
8. scoring method may be inadequate because ... 

## Future Directions

### Notes
1. Research Questions
2. Designing Principles
	1. developing a more systematic method for collecting and analyzing literature to find and select principles (same for questions)
3. Scoring Methods
	1. developing a more systematic method for scoring responses to the questionnaire
	2. experimenting with more scoring methods for the questionnaire
4. Instrument Analysis
5. Knowledge Delivery
	1. Improve Web UI
6. [cognitive interviews](https://files.eric.ed.gov/fulltext/EJ1272902.pdf)
	1. have more interviews with others to see if they can understand the questions

## Early Takeaways

### Notes

- Can mention generalizability?

## Thoughts from Piloting Questionnaire

# Prose

## Early Takeaways

### Generalizability

The DAO Index questionnaire was generally applicable to every DAO assessed.  At most, one to two questions were considered inapplicable in the draft assessments.

## Assumptions and Limitations
Our current work on the DAO Index, Version 0.9, faced the following assumptions and limitations:

1. An explicit assumption and limitation of our approach is our DAO definition.[^2] Our DAO definition inherently excludes organizations, that may be considered DAOs by others. Though, we hope to counter this issue by focusing on the principles.
2. The assessment takes an outside-in approach to assessing DAOs. Thus, we do not have complete information about the internal workings of the DAO, but only the publicly available information we can find provided by the DAO directly, or indirectly through third parties.
3. The principles we selected may not be representative of ideals for how a DAO should be operated and governed. In other words, our principles may not reflect the views of members of the DAO ecosystem, the public, and academia.
4. Our assessments were limited by the lack of standardized documentation, such as the Securities and Exchange Commission (SEC)'s standard for Form K-10. The lack of standardized documentation limited our efforts to identify potential sources for responses.
5. Our dataset of eleven assessments is a small dataset, that makes it difficult to truly generalize the results of our assessments, and to conduct instrument analysis on the DIAI.
6. Our methodology suffered from a lack of systematic research approach to developing the principles and questionnaire, which may have led to compromised results or the inability to interpret our results to determine reasonable outcomes.
7. Our scoring method may be inadequate for benchmarking and comparing DAOs%%need more prose here%%.
8. Some of the questions may not be appropriate for their principle.
9. As assessors, our own set of working knowledge may have hindered us from truly understanding how to interpret a question or understand certain materials in formulating a response to a question.
10. Link rot can also be an issue
11. Some of the questions (e.g., Banzhaf Power Index), will make more sens if measured periodically (or monitored constantly), rather than solely when we conduct an assessment

## Future Directions

Here, future work on the project will include:

1. Conducting inter-rater reliability checks
3. Developing a more scientifically-grounded/robust methodology for identifying and selecting the principles and questions for the questionnaire, and for scoring assessments
3. Developing a more scientifically-grounded/robust methodology  for scoring assessments
4. Improving our dashboard's UI/UX to make it easier to identify relevant resources and better use the ratings
5. Map the principles to organizational design dimensions
6. Compare the DAO Index with other DAO assessment frameworks
7. ~~Conducting test re-test reliability checks~~

## Thoughts from Piloting the Questionnaire

From piloting or testing out the questionnaire for DAO Index V0.9, we gained valuable feedback and learned many lessons.

Regarding questions

We found it difficult to respond to the HCAG questions, primarily because of how we constructed the questions. In other words, the questions lacked enough clarity to meaningfully respond, given the evidence we found. Thus, we realized we need to improve the clarity of the questions.

Additionally, that we may need more questions under HCAG to truly understand how this principle can guide the development and design of DAOs.

We received an interesting note from [guy from MakerDAO] on the use of the Banzhaf Power Index for BSP-02. [name] comment was that BSP-02 should take into account quorum settings for different voting scenarios. We did not consider this situation originally when we created the question, as we assumed that DAOs would use a single voting setting for their decision-making.

We found that the scoring method for responses was too harsh on DAOs. Though, we do seek to promote design structures and practices that are aligned with the ideal of DAOs, we were too harsh on DAOs that answered *No* to a question (i.e., received zero points), because if we can find information to respond to the question, that also promotes our goal of improving transparency. Thus, we realized that we need to update our scoring (or numeric scoring) for responses. 

We found that certain questions, such as [question-number] were likely compound questions (i.e., the question sought multiple answers). Thus, we realized that we need to split these questions into new questions in future versions.

# References

[^1]:  Brody, Ann. “DAOmeter: Our Research-Based Approach.” StableLab, 22 Mar. 2023, <https://www.stablelab.xyz/post/daometer-our-research-based-approach>.
[^2]: [The ins and outs of decentralized autonomous organizations (DAOs) unraveling the definitions, characteristics, and emerging developments of DAOs - ScienceDirect](https://www.sciencedirect.com/science/article/pii/S2096720923000180)
